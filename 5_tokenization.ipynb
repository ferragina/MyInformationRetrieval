{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferragina/MyInformationRetrieval/blob/main/5_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_a9QvUFVCUR"
      },
      "source": [
        "# Lesson 2: Comparing Trained LLM Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk5FlRFe-JdU"
      },
      "source": [
        "In this notebook of lesson 2, you will work with several tokenizers associated with different LLMs and explore how each tokenizer approaches tokenization differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5lcM1et-JdU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_QhwZ5c-JdU"
      },
      "source": [
        "We start with setting up the lab by installing the `transformers` library and ignoring the warnings. The requirements for this lab are already installed, so you don't need to uncomment the following cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgAnXp8N-JdU"
      },
      "source": [
        "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> ðŸ’» &nbsp; <b>Access <code>requirements.txt</code> file:</b> If you'd like to access the requirements file: 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix â€“ Tips, Help, and Download\"</em> Lesson.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "kD2LybB6-JdV"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers>=4.46.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 64,
        "id": "P3ja9cPH-JdV"
      },
      "outputs": [],
      "source": [
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cel1hybs-JdW"
      },
      "source": [
        "## Tokenizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw3Ec1ZD-JdW"
      },
      "source": [
        "In this section, you will tokenize the sentence \"Hello World!\" using the tokenizer of the [`bert-base-cased` model](https://huggingface.co/google-bert/bert-base-cased).\n",
        "\n",
        "Let's import the `Autotokenizer` class, define the sentence to tokenize, and instantiate the tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK8y39_g-JdW"
      },
      "source": [
        "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>FYI: </b> The transformers library has a set of Auto classes, like AutoConfig, AutoModel, and AutoTokenizer. The Auto classes are designed to automatically do the job for you.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "LxH-gN_n-JdW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "vuUbUWhj-JdX"
      },
      "outputs": [],
      "source": [
        "# define the sentence to tokenize\n",
        "sentence = \"Hello world!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "WmABy0Gt-JdX"
      },
      "outputs": [],
      "source": [
        "# load the pretrained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDTMOtAE-JdX"
      },
      "source": [
        "You'll now apply the tokenizer to the sentence. The tokeziner splits the sentence into tokens and returns the IDs of each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "dnBgfg71-JdX"
      },
      "outputs": [],
      "source": [
        "# apply the tokenizer to the sentence and extract the token ids\n",
        "token_ids = tokenizer(sentence).input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "fHQMADD2-JdX"
      },
      "outputs": [],
      "source": [
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhhn0iGO-JdX"
      },
      "source": [
        "To map each token ID to its corresponding token, you can use the `decode` method of the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "O4EHOEP5-JdX"
      },
      "outputs": [],
      "source": [
        "for id in token_ids:\n",
        "    print(tokenizer.decode(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj0h80xC-JdX"
      },
      "source": [
        "## Visualizing Tokenization\n",
        "\n",
        "In this section, you'll wrap the code of the previous section in the function `show_tokens`. The function takes in a text and the model name, and prints the vocabulary length of the tokenizer and a colored list of the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 421,
        "id": "7W0xFIVo5A0S"
      },
      "outputs": [],
      "source": [
        "# A list of colors in RGB for representing the tokens\n",
        "colors = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence: str, tokenizer_name: str):\n",
        "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
        "\n",
        "    # Load the tokenizer and tokenize the input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Vocab length: {len(tokenizer)}\")\n",
        "\n",
        "    # Print a colored list of tokens\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8md6lceK-JdY"
      },
      "source": [
        "Here's the text that you'll use to explore the different tokenization strategies of each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 115,
        "id": "-TXkEeZf-JdY"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "ðŸŽµ é¸Ÿ\n",
        "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrHj_R5G-JdY"
      },
      "source": [
        "You'll now again use the tokenizer of `bert-base-cased` and compare its tokenization strategy to that of `Xenova/gpt-4`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3l0rSUP-JdY"
      },
      "source": [
        "**bert-base-cased**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "fCDGSXP75Hv-"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwqfOg60-JdY"
      },
      "source": [
        "**Optional - bert-base-uncased**\n",
        "\n",
        "You can also try the uncased version of the bert model, and compare the vocab length and tokenization strategy of the two bert versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "0Ay_NX3K5HyP"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EqxoiwG-JdY"
      },
      "source": [
        "**GPT-4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "k-zTZY_F-JdY"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"Xenova/gpt-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_qUH9le-JdZ"
      },
      "source": [
        "### Optional Models to Explore\n",
        "\n",
        "You can also explore the tokenization strategy of other models. The following is a suggested list. Make sure to consider the following features when you're doing your comparison:\n",
        "- Vocabulary length\n",
        "- Special tokens\n",
        "- Tokenization of the tabs, special characters and special keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge6idLua-JdZ"
      },
      "source": [
        "**gpt2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "K_k5QduY5H0u"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybwp8u9z-JdZ"
      },
      "source": [
        "**Flan-T5-small**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "EJn5nf3c5H2_"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"google/flan-t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojq0T1-z-JdZ"
      },
      "source": [
        "**Starcoder 2 - 15B**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "3_vAyeTy5H7_"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bigcode/starcoder2-15b\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}