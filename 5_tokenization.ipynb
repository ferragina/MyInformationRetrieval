{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferragina/MyInformationRetrieval/blob/main/5_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_a9QvUFVCUR"
      },
      "source": [
        "# Comparing Trained LLM Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGO14Wno-0sZ"
      },
      "source": [
        "In this notebook, you will work with several tokenizers associated with different LLMs and explore how each tokenizer approaches tokenization differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE18q65u-0sZ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkgMeoI3-0sZ"
      },
      "source": [
        "We start with setting up the lab by installing the `transformers` library and ignoring the warnings. The requirements for this lab are already installed, so you don't need to uncomment the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "0mgfj7ff-0sa"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers>=4.46.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 64,
        "id": "CpRw83uL-0sb"
      },
      "outputs": [],
      "source": [
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-dYweMh-0sb"
      },
      "source": [
        "## Tokenizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekja6q5r-0sb"
      },
      "source": [
        "In this section, you will tokenize the sentence \"Hello World!\" using the tokenizer of the [`bert-base-cased` model](https://huggingface.co/google-bert/bert-base-cased).\n",
        "\n",
        "Let's import the `Autotokenizer` class, define the sentence to tokenize, and instantiate the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "LkIJgB0I-0sc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "WnNM9dJg-0sc"
      },
      "outputs": [],
      "source": [
        "# load the pretrained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "Oa-yXKE8-0sc"
      },
      "outputs": [],
      "source": [
        "# define the sentence to tokenize\n",
        "sentence = \"Hello world!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkdWdlBG-0sc"
      },
      "source": [
        "You'll now apply the tokenizer to the sentence. The tokeziner splits the sentence into tokens and returns the IDs of each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "HOWVHoHZ-0sc"
      },
      "outputs": [],
      "source": [
        "# apply the tokenizer to the sentence and extract the token ids\n",
        "token_ids = tokenizer(sentence).input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "o0bYdFCm-0sc"
      },
      "outputs": [],
      "source": [
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrBl7luS-0sc"
      },
      "source": [
        "To map each token ID to its corresponding token, you can use the `decode` method of the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "dmxG8K4w-0sc"
      },
      "outputs": [],
      "source": [
        "for id in token_ids:\n",
        "    print(tokenizer.decode(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6lHQPnS-0sd"
      },
      "source": [
        "## Visualizing Tokenization\n",
        "\n",
        "In this section, you'll wrap the code of the previous section in the function `show_tokens`. The function takes in a text and the model name, and prints the vocabulary length of the tokenizer and a colored list of the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 421,
        "id": "7W0xFIVo5A0S"
      },
      "outputs": [],
      "source": [
        "# A list of colors in RGB for representing the tokens\n",
        "colors = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence: str, tokenizer_name: str):\n",
        "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
        "\n",
        "    # Load the tokenizer and tokenize the input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Vocab length: {len(tokenizer)}\")\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Number of tokens: {len(token_ids)}\\n\")\n",
        "\n",
        "    # Print a colored list of tokens\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcJrlm0q-0sd"
      },
      "source": [
        "Here's the text that you'll use to explore the different tokenization strategies of each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 115,
        "id": "mqqqLPFx-0sd"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "ðŸŽµ é¸Ÿ\n",
        "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdhyahi6-0sd"
      },
      "source": [
        "You'll now again use the tokenizer of `bert-base-cased` and compare its tokenization strategy to that of `Xenova/gpt-4`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Olzc9n-0sd"
      },
      "source": [
        "**bert-base-cased**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "fCDGSXP75Hv-"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-dmPbLY-0sd"
      },
      "source": [
        "**bert-base-uncased**\n",
        "\n",
        "You can also try the uncased version of the bert model, and compare the vocab length and tokenization strategy of the two bert versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "0Ay_NX3K5HyP"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvE57bEE-0sd"
      },
      "source": [
        "**GPT-4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "nGG0s337-0se"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"Xenova/gpt-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56ZhXrgO-0se"
      },
      "source": [
        "### Optional Models to Explore\n",
        "\n",
        "You can also explore the tokenization strategy of other models. The following is a suggested list. Make sure to consider the following features when you're doing your comparison:\n",
        "- Vocabulary length\n",
        "- Special tokens\n",
        "- Tokenization of the tabs, special characters and special keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDG0wtAL-0se"
      },
      "source": [
        "**gpt2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "K_k5QduY5H0u"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqwHP2iy-0se"
      },
      "source": [
        "**Flan-T5-small**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "EJn5nf3c5H2_"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"google/flan-t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2randAE-0se"
      },
      "source": [
        "**Starcoder 2 - 15B**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "3_vAyeTy5H7_"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bigcode/starcoder2-15b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqVOfXvg-0se"
      },
      "source": [
        "**Phi-3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "__QNj2Cohzz2"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUtkHkzW-0se"
      },
      "source": [
        "**Qwen2 - Vision-Language Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "Aut598R8-0sh"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"Qwen/Qwen2-VL-7B-Instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aexW3vuV-0si"
      },
      "source": [
        "<p style=\"background-color:#f2f2ff; padding:15px; border-width:3px; border-color:#e2e2ff; border-style:solid; border-radius:6px\"> â¬‡\n",
        "&nbsp; <b>Download Notebooks:</b> If you'd like to donwload the notebook: 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>. For more help, please see the <em>\"Appendix â€“ Tips, Help, and Download\"</em> Lesson.</p>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}