{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferragina/MyInformationRetrieval/blob/main/5_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_a9QvUFVCUR"
      },
      "source": [
        "# Comparing Trained LLM Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGO14Wno-0sZ"
      },
      "source": [
        "In this notebook, we will work with some tokenizers associated with different LLMs and explore how each tokenizer approaches tokenization differently.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE18q65u-0sZ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "height": 30,
        "id": "0mgfj7ff-0sa"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers>=4.46.1\n",
        "\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-dYweMh-0sb"
      },
      "source": [
        "## Tokenizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekja6q5r-0sb"
      },
      "source": [
        "Let's import the `Autotokenizer` class, instantiate the tokenizer [`bert-base-cased` model](https://huggingface.co/google-bert/bert-base-cased), define the sentence to tokenize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "height": 30,
        "id": "LkIJgB0I-0sc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "height": 47,
        "id": "WnNM9dJg-0sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be77a8a-6d3e-4c6d-d425-c8a708d779b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of tokens in the dictionary:  28996\n"
          ]
        }
      ],
      "source": [
        "# load the pretrained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "print(\"number of tokens in the dictionary: \", len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "height": 47,
        "id": "Oa-yXKE8-0sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa41bc13-c89e-4d5e-cfb0-44daa75471ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 8667, 1362, 106, 102]\n"
          ]
        }
      ],
      "source": [
        "# define the sentence to tokenize\n",
        "sentence = \"Hello world!\"\n",
        "\n",
        "# apply the tokenizer to the sentence and extract the token ids\n",
        "token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "height": 47,
        "id": "dmxG8K4w-0sc",
        "outputId": "2ba8e907-8d34-4fae-9201-735ab62bdac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]\n",
            "Hello\n",
            "world\n",
            "!\n",
            "[SEP]\n"
          ]
        }
      ],
      "source": [
        "# map each token ID to its corresponding token, you can use the `decode` method of the tokenizer.\n",
        "\n",
        "for id in token_ids:\n",
        "    print(tokenizer.decode(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6lHQPnS-0sd"
      },
      "source": [
        "## Visualizing Tokenization\n",
        "\n",
        "Let's wrap the code of the previous section in the function `show_tokens`. The function takes in a text and the model name, and prints the vocabulary length of the tokenizer and a colored list of the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "height": 421,
        "id": "7W0xFIVo5A0S"
      },
      "outputs": [],
      "source": [
        "# A list of colors in RGB for representing the tokens\n",
        "colors = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence: str, tokenizer_name: str):\n",
        "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
        "\n",
        "    # Load the tokenizer and tokenize the input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"\\nVocab length: {len(tokenizer)}\")\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Number of tokens: {len(token_ids)}\\n\")\n",
        "\n",
        "    # Print a colored list of tokens\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try different tokenization\n",
        "\n",
        "Make sure to consider the following features when you're doing your comparison:\n",
        "- Vocabulary length\n",
        "- Special tokens\n",
        "- Tokenization of the tabs, special characters and special keywords\n",
        "\n"
      ],
      "metadata": {
        "id": "BAzX_xuMpLx0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "height": 115,
        "id": "mqqqLPFx-0sd"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "ðŸŽµ é¸Ÿ\n",
        "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Olzc9n-0sd"
      },
      "source": [
        "**bert-base-cased**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "fCDGSXP75Hv-"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-dmPbLY-0sd"
      },
      "source": [
        "**bert-base-uncased**\n",
        "\n",
        "You can also try the uncased version of the bert model, and compare the vocab length and tokenization strategy of the two bert versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "0Ay_NX3K5HyP"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvE57bEE-0sd"
      },
      "source": [
        "**GPT-4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "nGG0s337-0se"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"Xenova/gpt-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2randAE-0se"
      },
      "source": [
        "**Starcoder 2 - 15B**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "3_vAyeTy5H7_"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bigcode/starcoder2-15b\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}