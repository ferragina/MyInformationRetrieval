{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferragina/MyInformationRetrieval/blob/main/6_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM in practice**"
      ],
      "metadata": {
        "id": "euSpBGPyWT2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ehWhrBsKVeaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e0fb11-76a6-4547-ab23-ceb69baf6acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (1.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers>=4.41.2 accelerate>=0.31.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the required classes\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "WGv9nqvSYBb7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cpu\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "pXro7KtWYDue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating a Text Response to a Prompt\n",
        "\n",
        "You'll now use the pipeline object (labeled as generator) to generate a response consisting of 50 tokens to the given prompt."
      ],
      "metadata": {
        "id": "zEPAIgd-YZ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "d4jk5_N82-0s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = model.model(input_ids)\n",
        "\n",
        "model_output[0].shape\n",
        "\n",
        "# The first number represents the batch size, which is 1 in this case since we have one prompt.\n",
        "# The second number 5 represents the number of tokens.\n",
        "# And finally 3072 represents the embedding size (the size of the vector that corresponds to each token)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oXP17AT3E9j",
        "outputId": "f80007a3-cceb-4476-f486-e72914df516a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The transformer block outputs for each token a vector of size 3072 (embedding size). Let's check the shape of this output.\n",
        "\n",
        "lm_head_output = model.lm_head(model_output[0])\n",
        "\n",
        "lm_head_output.shape\n",
        "\n",
        "# The LM head outputs for each token in the input prompt, a vector of size 32064 (vocabulary size).\n",
        "# So there are 5 vectors, each of size 32064.\n",
        "# Each vector can be mapped to a probability distribution, that shows the probability for each token in the vocabulary to come after the given token in the input prompt."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwBA3-353LS4",
        "outputId": "135b518e-ccf5-462f-da02-858611fd7c3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32064])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're interested in generating the output token that comes after the last token in the input prompt (\"is\"), we'll focus on the last vector. So in the next cell, lm_head_output[0,-1] is a vector of size 32064 from which you can generate the token that comes after (\"is\"). You can do that by finding the id of the token that corresponds to the highest value in the vector lm_head_output[0,-1] (using argmax(-1), -1 means across the last axis here)."
      ],
      "metadata": {
        "id": "1RVu05Us5qIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = lm_head_output[0,-1].argmax(-1)\n",
        "\n",
        "tokenizer.decode(token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MmYwjCVu3PCp",
        "outputId": "037f94d9-fc61-478f-c27e-0e8271dc6457"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Paris'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}